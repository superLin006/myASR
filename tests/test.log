# 多语种翻译模型
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import torch
import psutil

# 函数：获取当前内存使用情况（以 GB 为单位）
def get_memory_usage():
    mem = psutil.virtual_memory()
    return mem.used / (1024 ** 3)

# 检查 CUDA 是否可用
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 打印初始内存使用情况
print(f"Initial Memory Usage: {get_memory_usage():.2f} GB")

# 指定模型路径
model_path = "../models/m2m100_418M"

# 加载分词器和模型
tokenizer = M2M100Tokenizer.from_pretrained(model_path)
model = M2M100ForConditionalGeneration.from_pretrained(model_path)

# 打印内存使用情况
print(f"After loading model, Memory Usage: {get_memory_usage():.2f} GB")

# 将模型移动到 GPU 并转换为半精度（可选）
model.to(device)
model.half()
print(f"After moving to GPU and half precision, Memory Usage: {get_memory_usage():.2f} GB")

# 设置源语言和目标语言
tokenizer.src_lang = "en"
target_language = "ja"

# 输入文本
text = "Strolling along the forest path, you can feel the fresh air and the sound of birdsong, accompanied by the leaves swaying gently in the wind, and the breeze brushing your face, bringing bursts of floral fragrance, which makes you feel relaxed and happy. In the sound of flowing mountain streams, you seem to hear the whispers of nature. It is a beauty beyond language that can allow people to escape from the hustle and bustle of the city and regain their inner peace. Whether sitting quietly under the mottled walls of an ancient temple or wandering through the busy streets of a city, you can experience the colorful life and the impermanence of the world in different scenes. Life is like a river, flowing endlessly in an ever-changing trajectory, and we are travelers floating on it. On the road to pursuing ideals and happiness, our steps may be slow or fast, light or heavy, but we are always full of hope, because every fall makes us stronger, and every time we look up, we see the light. As long as there is love in our hearts, we can shine our own light no matter where we are."

# 编码输入文本并移动到 GPU
encoded = tokenizer(text, return_tensors="pt").to(device)

# 打印内存使用情况
print(f"Before generation, Memory Usage: {get_memory_usage():.2f} GB")

# 生成翻译
with torch.no_grad():
    generated_tokens = model.generate(
        **encoded, 
        forced_bos_token_id=tokenizer.get_lang_id(target_language)
    )

# 将生成的翻译移动到 CPU 并解码
translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
print(translated_text)

# 打印最终内存使用情况
print(f"After generation, Memory Usage: {get_memory_usage():.2f} GB")







# 标点符号修复

# -*- coding: utf-8 -*-
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import os

# 1. 设备判断：优先使用 GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 本地模型路径
local_model_path = "../models/zh-wiki-punctuation-restore"

# 加载分词器和模型
tokenizer = AutoTokenizer.from_pretrained(local_model_path)
model = AutoModelForTokenClassification.from_pretrained(local_model_path)
model.to(device)
model.eval()

# 标签映射 id->label
label_map = model.config.id2label  # e.g. {0: "O", 1: "B-，", 2:"I-，", 3:"S-。", ...}

def restore_punctuation(text: str) -> str:
    # 2. 分词并搬到 device
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
    # [batch, seq_len, num_labels] -> [seq_len]
    predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()

    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"].squeeze())
    result = []

    for token, pred_id in zip(tokens, predictions):
        # 跳过 special tokens
        if token in tokenizer.all_special_tokens:
            continue

        # 合并子词
        if token.startswith("##"):
            result[-1] += token[2:]
        else:
            result.append(token)

        # 取出预测标签，跳过 "O"
        raw_label = label_map.get(pred_id, "O")
        if raw_label != "O":
            # 如果标签中有 '-', 'S-。' -> '。'
            if "-" in raw_label:
                punct = raw_label.split("-", 1)[1]
            else:
                punct = raw_label
            result[-1] += punct

    # 拼回字符串
    return "".join(result)

if __name__ == "__main__":
    text = (
    "漫步在林间小道上你可以感受清新的空气与鸟鸣声伴随着树叶在风中轻轻摇曳微风拂过脸庞带来阵阵花香让人心旷神怡在山间溪水流淌的声音中你仿佛听见大自然的低语那是一种超越语言的美可以让人走出喧嚣的都市重拾内心的宁静无论是静坐于古寺斑驳的墙壁下还是漫游于繁忙的市井街巷都能在不同的场景中体会到人生的多彩与世事的无常生命如同河流在不断变化的轨迹中奔流不息而我们则是漂浮其上的旅人在追寻理想与幸福的道路上脚步或慢或快或轻或重却始终充满希望因为每一次跌倒都让我们更加坚强每一次抬头都让我们看到光明只要心中有热爱无论身处何方都能绽放属于自己的光芒"
    )
    restored = restore_punctuation(text)
    print("原始：", text)
    print("恢复后：", restored)








# 翻译模型

from transformers import MarianMTModel, MarianTokenizer

# 模型名称（英文->中文）
model_name = "./Helsinki-NLP/opus-mt-zh-en"

# 加载 tokenizer 和模型
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# 输入文本
src_texts = ["你好很高兴认识你.", "，我是来自研发中心的算法工程师，我叫谢欢 ！."]

# 编码
inputs = tokenizer(src_texts, return_tensors="pt", padding=True)

# 翻译
translated = model.generate(**inputs)

# 解码结果
results = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
print(results)
